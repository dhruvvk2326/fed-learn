{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_22668\\2410525492.py:18: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv(file_path, dtype=dtypes, low_memory=False,\n",
      "C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_22668\\2410525492.py:18: UserWarning: Parsing dates in %d/%m/%Y %H:%M:%S format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df = pd.read_csv(file_path, dtype=dtypes, low_memory=False,\n"
     ]
    }
   ],
   "source": [
    "# ====== File Path Validation ======\n",
    "file_path = r\"C:\\Users\\dhruv\\Desktop\\project\\Fedrated_Privacy_Proj\\02-14-2018.csv\"\n",
    "assert os.path.exists(file_path), f\"File not found at {file_path}\"\n",
    "\n",
    "# ====== Memory-Optimized Loading ======\n",
    "dtypes = {\n",
    "    'Flow Duration': 'uint32',\n",
    "    'Tot Fwd Pkts': 'uint16',\n",
    "    'Flow Byts/s': 'float32',\n",
    "    'Flow Pkts/s': 'float32',\n",
    "    'Label': 'category'\n",
    "}\n",
    "\n",
    "# Load in chunks if memory constrained\n",
    "def process_chunk(chunk):\n",
    "    return chunk.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df = pd.read_csv(file_path, dtype=dtypes, low_memory=False, \n",
    "                 parse_dates=['Timestamp'], \n",
    "                 infer_datetime_format=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data Shape: (1048575, 80)\n",
      "Missing Values:\n",
      " Dst Port         0\n",
      "Protocol         0\n",
      "Timestamp        0\n",
      "Flow Duration    0\n",
      "Tot Fwd Pkts     0\n",
      "                ..\n",
      "Idle Mean        0\n",
      "Idle Std         0\n",
      "Idle Max         0\n",
      "Idle Min         0\n",
      "Label            0\n",
      "Length: 80, dtype: int64\n",
      "Label Categories: ['Benign', 'FTP-BruteForce', 'SSH-Bruteforce']\n",
      "Categories (3, object): ['Benign', 'FTP-BruteForce', 'SSH-Bruteforce']\n"
     ]
    }
   ],
   "source": [
    "# ====== Critical Column Checks ======\n",
    "print(\"Initial Data Shape:\", df.shape)\n",
    "print(\"Missing Values:\\n\", df.isna().sum())\n",
    "print(\"Label Categories:\", df['Label'].unique())\n",
    "\n",
    "# ====== Infinite Value Handling ======\n",
    "inf_cols = ['Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Max', 'Idle Max']\n",
    "df[inf_cols] = df[inf_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Protocol-aware imputation\n",
    "for col in inf_cols:\n",
    "    df[col] = df.groupby('Protocol', observed=True)[col].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "\n",
    "# ====== Irrelevant Column Removal ======\n",
    "cols_to_drop = [\n",
    "    'Timestamp', 'Fwd URG Flags', 'Bwd URG Flags', \n",
    "    'Init Fwd Win Byts', 'Init Bwd Win Byts'\n",
    "]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "# ====== Categorical Conversion ======\n",
    "df['Protocol'] = df['Protocol'].astype('category').cat.codes  # TCP=0, UDP=1\n",
    "df['Dst Port'] = df['Dst Port'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Robust Scaling ======\n",
    "scaler = RobustScaler(quantile_range=(5, 95), \n",
    "                      with_centering=False,  # Avoid negative values\n",
    "                      unit_variance=True)\n",
    "\n",
    "robust_features = [\n",
    "    'Flow Byts/s', 'Flow Pkts/s',\n",
    "    'Flow IAT Max', 'Idle Max'\n",
    "]\n",
    "\n",
    "# Ensure float32 to prevent overflow\n",
    "df[robust_features] = df[robust_features].astype('float32')\n",
    "\n",
    "# Quantile-based clipping (prevent post-scaling outliers)\n",
    "for col in robust_features:\n",
    "    q1 = df[col].quantile(0.05)\n",
    "    q3 = df[col].quantile(0.95)\n",
    "    df[col] = np.clip(df[col], q1, q3)\n",
    "\n",
    "# Apply scaling\n",
    "df[robust_features] = scaler.fit_transform(df[robust_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruv\\miniconda3\\envs\\fed-learn\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# ====== Federated Client Simulation ======\n",
    "# Strategy 1: Split by protocol type\n",
    "tcp_data = df[df['Protocol'] == 0].sample(frac=0.5, random_state=42)\n",
    "udp_data = df[df['Protocol'] == 1].sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Strategy 2: Temporal splitting (using original timestamp order)\n",
    "df_sorted = df.sort_values('Flow Duration')\n",
    "client_count = 5\n",
    "client_datasets = np.array_split(df_sorted, client_count)\n",
    "\n",
    "# Save partitions\n",
    "for i, client_df in enumerate(client_datasets):\n",
    "    client_df.to_parquet(\n",
    "        f'client_{i}.parquet',\n",
    "        engine='pyarrow',\n",
    "        compression='ZSTD'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Memory Usage: 0.5572707038372755 GB\n"
     ]
    }
   ],
   "source": [
    "# ====== Post-Processing Verification ======\n",
    "assert not df[robust_features].isnull().any().any(), \"NaNs present!\"\n",
    "assert not np.isinf(df[robust_features]).any().any(), \"Infinite values!\"\n",
    "assert df[robust_features].max().max() < 100, \"Scaling overflow\"\n",
    "assert df[robust_features].min().min() >= 0, \"Negative scaled values\"\n",
    "\n",
    "# Label distribution check\n",
    "label_dist = df['Label'].value_counts(normalize=True)\n",
    "assert label_dist.min() > 0.01, \"Severe class imbalance remains\"\n",
    "\n",
    "# Memory check (target <4GB)\n",
    "print(\"Final Memory Usage:\", df.memory_usage().sum()/1024**3, \"GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robust_scaler.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save processed dataset\n",
    "df.to_parquet('processed_data.parquet', \n",
    "             engine='pyarrow',\n",
    "             compression='ZSTD',\n",
    "             index=False)\n",
    "\n",
    "# Save scaler for federated clients\n",
    "import joblib\n",
    "joblib.dump(scaler, 'robust_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_feature_selection.py\n",
    "import pygad\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "from flwr.common.parameter import parameters_to_ndarrays\n",
    "\n",
    "class FedChimpStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(self, results, failures):\n",
    "        # Aggregate feature masks from clients\n",
    "        all_masks = [parameters_to_ndarrays(r.parameters)[0] for r in results]\n",
    "        global_mask = np.mean(all_masks, axis=0) > 0.5  # Majority voting\n",
    "        return fl.common.ndarrays_to_parameters([global_mask.astype(int)])\n",
    "\n",
    "def chimp_optimization(X_client):\n",
    "    # Windows-compatible ChOA implementation\n",
    "    ga = pygad.GA(\n",
    "        num_generations=20,\n",
    "        num_parents_mating=5,\n",
    "        fitness_func=lambda sol, _: fitness(sol, X_client),\n",
    "        gene_type=int,\n",
    "        gene_space=[0, 1],\n",
    "        suppress_warnings=True  # Windows console compatibility\n",
    "    )\n",
    "    ga.run()\n",
    "    return ga.best_solution()[0]\n",
    "\n",
    "def fitness(solution, X):\n",
    "    selected = X.columns[np.where(solution == 1)[0]]\n",
    "    return X[selected].var().sum()  # Maximize feature variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
